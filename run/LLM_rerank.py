import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig
import json
from tqdm import tqdm
import gc

# ëª¨ë¸ ì´ë¦„ê³¼ ê²½ë¡œ ì„¤ì •
def init_model():
    """
    Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    CPU ì˜¤í”„ë¡œë”© ì—†ì´, ëª¨ë¸ ì „ì²´ë¥¼ ë‹¨ì¼ GPUì— ì˜¬ë¦¬ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
    """
    model_name = "K-intelligence/Midm-2.0-Base-Instruct"

    # 4ë¹„íŠ¸ ì–‘ìí™”
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        # quantization_config=quantization_config,
        device_map="cuda:0", # "auto" ëŒ€ì‹  íŠ¹ì • GPUë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    generation_config = GenerationConfig.from_pretrained(model_name)

    print("ëª¨ë¸ ë¡œë”© ë° íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ! (ë‹¨ì¼ GPU)")
    return model,tokenizer,generation_config

# --- 1ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼) ---
def load_full_dataset(file_path: str) -> list:
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {file_path}")
    try:
        with open(file_path, "r", encoding='utf-8') as f:
            dataset = json.load(f)
        print(f" ì´ {len(dataset)}ê°œì˜ í•­ëª©ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        return dataset
    except Exception as e:
        print(f" ì˜¤ë¥˜: ë°ì´í„°ì…‹ ë¡œë”© ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")
        return []

# --- 2ë‹¨ê³„: ìµœì ì˜ ë¬¸ì„œ ì§‘í•© ì„ íƒ ---
def select_best_document_indices(query: str, candidate_docs: list[str], model,tokenizer,generation_config) -> list[int]:
    formatted_documents = "\n".join([f"[ë¬¸ì„œ {i}]: {doc}" for i, doc in enumerate(candidate_docs)])
    prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì§ˆë¬¸]ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì„ [í›„ë³´ ë¬¸ì„œ] ì¤‘ì—ì„œ ì„ íƒí•˜ëŠ” í•œêµ­ì–´ ì–¸ì–´ ê·œë²” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ê³  ê³¼ì •ì— ë”°ë¼, ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ë²ˆí˜¸ë¥¼ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 

1. **í•µì‹¬ ì›ë¦¬ íŒŒì•…**: [ì§ˆë¬¸]ì„ í•´ê²°í•˜ëŠ” ë° í•„ìš”í•œ ë¬¸ë²•ì  ì›ë¦¬ê°€ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•©ë‹ˆë‹¤. ë§Œì•½ ì™¸êµ­ì–´ì— ëŒ€í•œ í‘œê¸°ë²•ì¼ ê²½ìš°ì—ëŠ” ì™¸ë˜ì–´ í‘œê¸°ë²•ì— ê´€ë ¨ëœ ì¡°í•­ì„ ì‚´í´ì„œ íŒŒì•…í•©ë‹ˆë‹¤.
2. **ë¬¸ì„œ ë§¤í•‘**: íŒŒì•…ëœ ì›ë¦¬ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ë¬¸ì„œë¥¼ [í›„ë³´ ë¬¸ì„œ]ì—ì„œ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤. ë¬¸ì„œì— ëŒ€í•œ ì„¤ëª…ì˜ ê²½ìš° ì´ ë¶€ë¶„ì—ì„œ ì„¤ëª…í•˜ë„ë¡ í•©ë‹ˆë‹¤.
3. **ìµœì¢… ì„ íƒ**: ì°¾ì€ ë¬¸ì„œë“¤ì˜ ë²ˆí˜¸ë¥¼ `### ìµœì¢… ì„ íƒ:` í˜•ì‹ìœ¼ë¡œ ì˜¤ë¡¯ì´ ë¬¸ì„œ ë²ˆí˜¸ë§Œì„ í•œ ì¤„ì— ì¶œë ¥í•©ë‹ˆë‹¤. í›„ë³´ë¬¸ì„œ ì¤‘ì—ì„œ ì •ë‹µë¬¸ì„œë¥¼ ë°˜ë“œì‹œ í•œ ê°œ ì´ìƒ ì„¸ ê°œ ì´í•˜ë¡œ ì„ íƒí•´ì„œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ìµœì¢… ì„ íƒì—ì„œ ì•„ë¬´ê²ƒë„ ì„ íƒí•˜ì§€ ì•ŠëŠ” ê²ƒì€ ì•ˆë©ë‹ˆë‹¤.
"""

    user_input = f"**[ì§ˆë¬¸]** {query} **[í›„ë³´ ë¬¸ì„œ]**{formatted_documents} "
    message = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_input},
            ]
    source = tokenizer.apply_chat_template(
                    message,
                    tokenize = True,
                    force_reasoning=True,
                    add_generation_prompt=True,
                    return_tensors="pt",
                )
    outputs = model.generate(
                            source.to("cuda:0"),
                            generation_config=generation_config,
                            eos_token_id=tokenizer.eos_token_id,
                            max_new_tokens=1024,
                            do_sample=False,
                        )
    generated_texts = []
    for output in outputs:
            text = tokenizer.decode(output, skip_special_tokens=False)
            generated_texts.append(text)
    assistant_response = generated_texts[0].split("<|start_header_id|>assistant<|end_header_id|>")[1].strip()
    print(assistant_response)
    selection_line = assistant_response.split('### ìµœì¢… ì„ íƒ:')[1].strip()
    selection_line = selection_line.split("**ì„¤ëª…")[0]
    selection_line = selection_line.split("**ìˆ˜ì •")[0]
    selection_line = selection_line.split("\n")[0]
    selection_line = selection_line.replace("ë¬¸ì„œ","").strip()
    selection_line = selection_line.replace(" ###","").strip()
    selection_line = selection_line.replace("```","").strip()
    selection_line = selection_line.replace("[","").strip()
    selection_line = selection_line.replace("]","").strip()
    selection_line = selection_line.replace("*","").strip()
    selection_line = selection_line.replace("<|end_of_text|>","").strip()
    selection_line = selection_line.replace(", "," ").strip()
    selection_line = selection_line.replace("\n"," ").strip()
    selection_doc = selection_line.split(" ")
    print(selection_doc)
    print("seletion_line",len(selection_doc))
    

    selected_indices_str = selection_doc
    return [int(i) for i in selected_indices_str if i != ""]
    # return [i for i in selected_indices_str]


# --- ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ ---
def save_results_to_json(results: list, output_file_path: str):
    print(f"\n ì´ {len(results)}ê°œì˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤: {output_file_path}")
    try:
        with open(output_file_path, "w", encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(" íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ë°°ì¹˜ í¬ê¸° 1 ë³´ì¥ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì •ë¦¬ í¬í•¨) ---
def main():
    model,tokenizer,generation_config = init_model()
    
    # input_file_path = "/home/nlplab/ssd2/jeong/RAG/GCU-koreanRAG/data/ë¯¿ìŠµë‹ˆë‹¤_3_chunks_test_0.6_qwen8b_k25.json"
    input_file_path = "../result/ë¯¿ì˜¤ì—†5_ì¿¼ë¦¬ë¶„í• 20_top25.json"

    dataset = load_full_dataset(input_file_path)

    if not dataset:
        print("ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    all_results = []
    
    print("\n" + "="*80)
    print("ğŸš€ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ Set Selection(Reranking)ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë°°ì¹˜ í¬ê¸° 1)")
    print("="*80)
    
    for item in tqdm(dataset, desc="Processing Queries"):
        # query = item.get("question") or item.get("query")
        query = item.get("answer") or item.get("query")
        original_chunks = item.get("retrieved_chunks", [])
        
        if not query or not original_chunks:
            continue

        candidate_docs_text = [chunk['text'] for chunk in original_chunks]
        selected_indices = select_best_document_indices(
            query, candidate_docs_text, model,tokenizer,generation_config
        )
        selected_texts = [candidate_docs_text[i] for i in selected_indices if i < 25]
        result_item = {
            "question": query,
            "answer": item.get("answer"),
            "reranking_result": {
                "selected_indices": selected_indices,
                "selected_texts": selected_texts
            },
            "original_retrieved_chunks": original_chunks
        }
        all_results.append(result_item)

            # gc.collect()
            # torch.cuda.empty_cache()

    output_file_path = "../result/reranking_results_test.json"
    save_results_to_json(all_results, output_file_path)

if __name__ == "__main__":
    main()